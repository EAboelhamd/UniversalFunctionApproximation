<b>Universal Function Approximation by Neural Nets</b>

The universal function approximation property of multilayer perceptrons was first noted by Cybenko (1989) and Hornik (1991): George Cybenko (1989),  “Approximations by superpositions of sigmoidal functions”,  Mathematics of Control, Signals, and Systems
Kurt Hornik (1991),  “Approximation Capabilities of Multilayer Feedforward Networks”,  Neural Networks

<i>“The universal approximation theorem states that a feed-forward network with a single hidden layer containing a finite number of neurons (i.e., a multilayer perceptron), can approximate continuous functions on compact subsets of Rn, under mild assumptions on the activation function. “</i>

<i>“The theorem thus states that simple neural networks can represent a wide variety of interesting functions when given appropriate parameters; however, it does not touch upon the algorithmic learnability of those parameters.”</i>

This repository contains Jupyter notebooks that use neural networks to learn arbitrary Python lambda expressions and WaveLets.

![Alt text](images/PythonLambdaResults.png?raw=true "")
